(9432, 224, 224, 3)
(9432, 224, 224, 3)
(?, 224, 224, 64)
(?, 112, 112, 128)
(?, 56, 56, 256)
(?, 28, 28, 512)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 224, 224, 3)  0                                            
__________________________________________________________________________________________________
block1_conv1 (Conv2D)           (None, 224, 224, 64) 1792        input_1[0][0]                    
__________________________________________________________________________________________________
block1_conv2 (Conv2D)           (None, 224, 224, 64) 36928       block1_conv1[0][0]               
__________________________________________________________________________________________________
block1_pool (MaxPooling2D)      (None, 112, 112, 64) 0           block1_conv2[0][0]               
__________________________________________________________________________________________________
block2_conv1 (Conv2D)           (None, 112, 112, 128 73856       block1_pool[0][0]                
__________________________________________________________________________________________________
block2_conv2 (Conv2D)           (None, 112, 112, 128 147584      block2_conv1[0][0]               
__________________________________________________________________________________________________
block2_pool (MaxPooling2D)      (None, 56, 56, 128)  0           block2_conv2[0][0]               
__________________________________________________________________________________________________
block3_conv1 (Conv2D)           (None, 56, 56, 256)  295168      block2_pool[0][0]                
__________________________________________________________________________________________________
block3_conv2 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv1[0][0]               
__________________________________________________________________________________________________
block3_conv3 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv2[0][0]               
__________________________________________________________________________________________________
block3_pool (MaxPooling2D)      (None, 28, 28, 256)  0           block3_conv3[0][0]               
__________________________________________________________________________________________________
block4_conv1 (Conv2D)           (None, 28, 28, 512)  1180160     block3_pool[0][0]                
__________________________________________________________________________________________________
block4_conv2 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv1[0][0]               
__________________________________________________________________________________________________
block4_conv3 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv2[0][0]               
__________________________________________________________________________________________________
block4_pool (MaxPooling2D)      (None, 14, 14, 512)  0           block4_conv3[0][0]               
__________________________________________________________________________________________________
block5_conv1 (Conv2D)           (None, 14, 14, 512)  2359808     block4_pool[0][0]                
__________________________________________________________________________________________________
block5_conv2 (Conv2D)           (None, 14, 14, 512)  2359808     block5_conv1[0][0]               
__________________________________________________________________________________________________
block5_conv3 (Conv2D)           (None, 14, 14, 512)  2359808     block5_conv2[0][0]               
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 28, 28, 512)  2359808     block5_conv3[0][0]               
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 28, 28, 512)  2048        conv2d_transpose_1[0][0]         
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 28, 28, 1024) 0           batch_normalization_1[0][0]      
                                                                 block4_conv3[0][0]               
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 28, 28, 512)  524800      concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 56, 56, 256)  1179904     conv2d_1[0][0]                   
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 56, 56, 256)  1024        conv2d_transpose_2[0][0]         
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 56, 56, 512)  0           batch_normalization_2[0][0]      
                                                                 block3_conv3[0][0]               
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 56, 56, 256)  131328      concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_3 (Conv2DTrans (None, 112, 112, 128 295040      conv2d_2[0][0]                   
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 112, 112, 128 512         conv2d_transpose_3[0][0]         
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 112, 112, 256 0           batch_normalization_3[0][0]      
                                                                 block2_conv2[0][0]               
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 112, 112, 128 32896       concatenate_3[0][0]              
__________________________________________________________________________________________________
conv2d_transpose_4 (Conv2DTrans (None, 224, 224, 64) 73792       conv2d_3[0][0]                   
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 224, 224, 64) 256         conv2d_transpose_4[0][0]         
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 224, 224, 128 0           batch_normalization_4[0][0]      
                                                                 block1_conv2[0][0]               
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 224, 224, 64) 8256        concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 224, 224, 3)  195         conv2d_4[0][0]                   
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 224, 224, 6)  0           conv2d_5[0][0]                   
                                                                 input_1[0][0]                    
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 224, 224, 3)  21          concatenate_5[0][0]              
==================================================================================================
Total params: 19,324,568
Trainable params: 6,967,768
Non-trainable params: 12,356,800
__________________________________________________________________________________________________
Train on 7545 samples, validate on 1887 samples
Epoch 1/20

  32/7545 [..............................] - ETA: 37:45 - loss: 7.5618 - psnr: -5.7760
  64/7545 [..............................] - ETA: 21:18 - loss: 6.3378 - psnr: -4.9266
  96/7545 [..............................] - ETA: 15:50 - loss: 5.3728 - psnr: -4.0706
 128/7545 [..............................] - ETA: 13:04 - loss: 4.6974 - psnr: -3.3671
 160/7545 [..............................] - ETA: 11:24 - loss: 4.2165 - psnr: -2.8125
 192/7545 [..............................] - ETA: 10:17 - loss: 3.7868 - psnr: -2.1993
 224/7545 [..............................] - ETA: 9:29 - loss: 3.4946 - psnr: -1.7991 
 256/7545 [>.............................] - ETA: 9:08 - loss: 3.2496 - psnr: -1.4306
 288/7545 [>.............................] - ETA: 8:38 - loss: 3.0461 - psnr: -1.1056
 320/7545 [>.............................] - ETA: 8:13 - loss: 2.8302 - psnr: -0.6419
 352/7545 [>.............................] - ETA: 7:53 - loss: 2.6431 - psnr: -0.2079
 384/7545 [>.............................] - ETA: 7:36 - loss: 2.4874 - psnr: 0.1527 
 416/7545 [>.............................] - ETA: 7:21 - loss: 2.3519 - psnr: 0.4797
 448/7545 [>.............................] - ETA: 7:08 - loss: 2.2367 - psnr: 0.7542
 480/7545 [>.............................] - ETA: 6:57 - loss: 2.1358 - psnr: 0.9981
 512/7545 [=>............................] - ETA: 6:47 - loss: 2.0403 - psnr: 1.2590
 544/7545 [=>............................] - ETA: 6:38 - loss: 1.9502 - psnr: 1.5349
 576/7545 [=>............................] - ETA: 6:30 - loss: 1.8703 - psnr: 1.7787
 608/7545 [=>............................] - ETA: 6:23 - loss: 1.7992 - psnr: 1.9933
 640/7545 [=>............................] - ETA: 6:16 - loss: 1.7347 - psnr: 2.1900
 672/7545 [=>............................] - ETA: 6:10 - loss: 1.6768 - psnr: 2.3649
 704/7545 [=>............................] - ETA: 6:04 - loss: 1.6209 - psnr: 2.5539
 736/7545 [=>............................] - ETA: 5:59 - loss: 1.5696 - psnr: 2.7282
 768/7545 [==>...........................] - ETA: 5:53 - loss: 1.5212 - psnr: 2.9015
 800/7545 [==>...........................] - ETA: 5:49 - loss: 1.4765 - psnr: 3.0640
 832/7545 [==>...........................] - ETA: 5:44 - loss: 1.4335 - psnr: 3.2333
 864/7545 [==>...........................] - ETA: 5:40 - loss: 1.3948 - psnr: 3.3772
 896/7545 [==>...........................] - ETA: 5:36 - loss: 1.3575 - psnr: 3.5266
 928/7545 [==>...........................] - ETA: 5:32 - loss: 1.3221 - psnr: 3.6741
 960/7545 [==>...........................] - ETA: 5:28 - loss: 1.2899 - psnr: 3.8019
 992/7545 [==>...........................] - ETA: 5:25 - loss: 1.2591 - psnr: 3.9287
1024/7545 [===>..........................] - ETA: 5:22 - loss: 1.2291 - psnr: 4.0644
1056/7545 [===>..........................] - ETA: 5:18 - loss: 1.2016 - psnr: 4.1814
1088/7545 [===>..........................] - ETA: 5:15 - loss: 1.1739 - psnr: 4.3197
1120/7545 [===>..........................] - ETA: 5:12 - loss: 1.1487 - psnr: 4.4348
1152/7545 [===>..........................] - ETA: 5:09 - loss: 1.1242 - psnr: 4.5550
1184/7545 [===>..........................] - ETA: 5:06 - loss: 1.1007 - psnr: 4.6737
1216/7545 [===>..........................] - ETA: 5:04 - loss: 1.0791 - psnr: 4.7759
1248/7545 [===>..........................] - ETA: 5:01 - loss: 1.0584 - psnr: 4.8752
1280/7545 [====>.........................] - ETA: 4:58 - loss: 1.0383 - psnr: 4.9774
1312/7545 [====>.........................] - ETA: 4:56 - loss: 1.0202 - psnr: 5.0583
1344/7545 [====>.........................] - ETA: 4:53 - loss: 1.0013 - psnr: 5.1621
1376/7545 [====>.........................] - ETA: 4:51 - loss: 0.9837 - psnr: 5.2556
1408/7545 [====>.........................] - ETA: 4:48 - loss: 0.9683 - psnr: 5.3213
1440/7545 [====>.........................] - ETA: 4:46 - loss: 0.9530 - psnr: 5.3929
1472/7545 [====>.........................] - ETA: 4:44 - loss: 0.9379 - psnr: 5.4681
1504/7545 [====>.........................] - ETA: 4:42 - loss: 0.9239 - psnr: 5.5339
1536/7545 [=====>........................] - ETA: 4:39 - loss: 0.9094 - psnr: 5.6152
1568/7545 [=====>........................] - ETA: 4:37 - loss: 0.8956 - psnr: 5.6903
1600/7545 [=====>........................] - ETA: 4:35 - loss: 0.8824 - psnr: 5.7619
1632/7545 [=====>........................] - ETA: 4:33 - loss: 0.8696 - psnr: 5.8342
1664/7545 [=====>........................] - ETA: 4:31 - loss: 0.8568 - psnr: 5.9126
1696/7545 [=====>........................] - ETA: 4:29 - loss: 0.8448 - psnr: 5.9815
1728/7545 [=====>........................] - ETA: 4:27 - loss: 0.8332 - psnr: 6.0480
1760/7545 [=====>........................] - ETA: 4:25 - loss: 0.8217 - psnr: 6.1198
1792/7545 [======>.......................] - ETA: 4:23 - loss: 0.8108 - psnr: 6.1866
1824/7545 [======>.......................] - ETA: 4:21 - loss: 0.8005 - psnr: 6.2435
1856/7545 [======>.......................] - ETA: 4:19 - loss: 0.7909 - psnr: 6.2940
1888/7545 [======>.......................] - ETA: 4:17 - loss: 0.7810 - psnr: 6.3540
1920/7545 [======>.......................] - ETA: 4:15 - loss: 0.7716 - psnr: 6.4086
1952/7545 [======>.......................] - ETA: 4:14 - loss: 0.7624 - psnr: 6.4634
1984/7545 [======>.......................] - ETA: 4:12 - loss: 0.7533 - psnr: 6.5211
2016/7545 [=======>......................] - ETA: 4:10 - loss: 0.7450 - psnr: 6.5684
2048/7545 [=======>......................] - ETA: 4:08 - loss: 0.7364 - psnr: 6.6224
2080/7545 [=======>......................] - ETA: 4:06 - loss: 0.7283 - psnr: 6.6713